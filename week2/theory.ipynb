{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Theoretical Exercises week 2\n", "**Like last week, it is very imporant that you try to solve every exercise. It is not important that you answer correctly. Spend no more than 5-10 min on each exercise. If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n", "\n", "The TA's will be very happy to answer questions during the TA session or on the board.\n", "\n", "You might want to spend a bit more than 10 min on 5, 6 and 8, they are substantially harder than the rest of the exercises. Question 8 is mainly for the interested student with time left over. Do not despair if you cannot solve them, but try to understand the question and pinpoint which parts you do not understand. \n", "\n", "# 1. Learning Types\n", "## A shorty story about Ben \n", "Ben is a student at Aarhus University. Like all other university students Ben has to learn a lot of different stuff. Ben is taking a class called '5 ECTS ON BIRDS' where he has to learn two things\n", "\n", "   1. During the course, the lecturer gave Ben a photo album with images of different birds and their name. Ben needs to learn the names of all the birds. At the exam, Ben will be given several images for which he needs to name the birds.  \n", "    \n", "   2. During the course, the lecturer gave Ben several recordings of birds singing. For this task the lecturer doesn't care about names. At the exam, Ben will be given recordings he needs to seperate into two groups based on how similar they sound.    \n", "    \n", "Ben notices one significant difference between the two tasks. In the first, his supervisor gave him the correct answer for each image. This is called *supervised learning* because the supervisor provides a correct answer. In the second task, his supervisor did not give him a correct answer. Instead, Ben had to group the audio samples based on similarity. This is called *unsupervised learning*, because there is not supervision in terms of correct answers. \n", "\n", "In this exercise you must distinguish between Supervised Learning and Unsupervised Learning. Imagine you work at a company that sells *stuff*. The company stores a lot of information on its costumers. For each costumer the company saves the following 5 attributes:\n", "\n", "    AGE, SEX, INCOME, RESIDENCE, MONEY USED AT COMPANY\n", "\n", "<b>Question 1: </b><br>In each of the following examples you should determine if the problem is a Supervised or Unsupervised learning problem.\n", "\n", "-   The company wants to learn how to predict 'MONEY USED AT COMPANY' given 'AGE', 'SEX', 'INCOME' and 'RESIDENCE'. Supervised or Unsupervised?\n", "\n", "-   The company wants to learn ways of grouping costumers depending on 'AGE'. Supervised or Unsupervised?\n", "\n", "-   The company wants to learn how to predict 'SEX' given 'MONEY SPENT AT COMPANY' and 'AGE'. Supervised or Unsupervised?\n", "\n", "-   The company wants to target different groups of costumers depending on 'AGE', 'INCOME' and 'MONEY SPENT AT COMPANY'. Supervised or Unsupervised?   \n", "\n", "<br><br>\n", "<b>Question 2</b>:<br>\n", "In supervised learning the data is of the form $D_{supervised}=\\{(x_1,y_1),...,(x_n,y_n)\\}$. <br>\n", "In unsupervised learning we have data of the form $D_{unsupervised}=\\{x_1,...,x_n\\}$.\n", "\n", "Write the form the data would take in each case from Question 1.\n", "\n", "HINT: Possible solutions to two of the cases\n", "\n", "$$D=\\{20\\text{ years},\\;21\\text{ years}, \\;23\\text{ years}, ... \\}$$ \n", "$$D=\\{([100\\text{ kr},\\;22\\text{ years}],\\text{ male}),\\;([120\\text{ kr},\\;30\\text{ years}],\\text{ female}), ...\\}$$\n", " \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Regression Or Classification\n", "## The short story of Ben continues\n", "After passing the exam in '5 ECTS ON BIRDS' Ben goes on to take the prestigious '5 ECTS ON ADVANCED BIRDS'. In this class, Ben has to do the following two things\n", "\n", "   1. He is given a photo album with images of birds and the family they belong to. At the exam, Ben will get images of birds and asked to tell which bird family they belong to. \n", "    \n", "   2. He is given a photo album with images of birds and how much they cost. At the exam, Ben will get images of birds and be asked to tell how much the bird would cost to buy. \n", "    \n", "First of, Ben notices that the two tasks are supervised learning; he gets examples with correct answers. Ben also notices a significant difference between the two tasks. In the first, he has to assign each bird to a class. This is called *classification*. In the second task, he has to assign a real number to each bird. This is called *regression*. The difference is essentially that in regression it makes to estimate a value close to the actual value, while in classification you are either right or wrong and close to right is not really meaningfull.\n", "\n", "\n", "<b>Question 1</b>: <br>In each of the following examples you should distinguish between regression and classification.\n", "\n", "-   In the previous question the company wanted to predict 'MONEY SPENT AT COMPANY' from ('AGE', 'SEX', 'INCOME', 'RESIDENCE'). Is that regression or classification?\n", "\n", "-   Recognizing the color of wine as white, rose or red. Is that regression or classification?\n", "\n", "-   Predicting a students grade in machine learning as a function of previous grades (on the 12 scale). Is that regression or classification? \n", "    \n", "-   Predicting industry codes from purpose statements. Regression or classification?\n", "<br><br>\n", "    \n", "<b>Question 2: </b> <br>\n", "In supervised learning we want to approximate an unkown target function $f:X\\rightarrow Y$. In regression we could have $Y=\\mathbb{R}$ and in classification we could have $Y=\\{c_1,...,c_k\\}$.\n", "\n", "What is $Y$ in the above four cases? \n", "\n", "HINT: Consider $Y=\\{white, rose, red\\}$, $Y=\\{-3, 0, 2, 4, 7, 10, 12\\}$, $Y=\\mathbb{R}$. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Helping the Police catch bad guys (learning vs design) \n", "The police would like your help with criminal facial detection. They believe it is possible to classify whether a person is guilty by looking at the persons face. They want you to build a system that does that. To do this they give you a lot of data. The data is of the form\n", "\n", "    (MUGSHOT, GUILTY)\n", "\n", "Here '<a href=\"https://en.wikipedia.org/wiki/Mug_shot\">MUGSHOT</a>' is an image and 'GUILTY' is a boolean indicating if the person was guilty or not.  Unfortunately, all the mug shots were taken at a <a href=\"http://www.zastavki.com/pictures/originals/2014/Winter_Snow_in_Paris_Eiffel_Tower_tilted_056634_.jpg\">45 degree angle</a>. \n", "\n", "Your job is to build a classifier system that determines guilt from facial images. The system you construct has 3 different steps.\n", "\n", "<div style=\"border: 1px solid #333; padding: 16px; margin: 16px;\">\n", "<b>Step 1:</b>\n", "You write an algorithm that rotates the face -45 degrees so all suspects faces outwards.<br/><br/>\n", "\n", "<b>Step 2:</b>\n", "After looking at the rotated images you discover that the position of the eyes is always 3cm down in the picture. You decide to build a <a href=\"https://en.wikipedia.org/wiki/Feature_extraction\">feature extractor</a> that looks 3cm down in the image and locates the\n", "eyes. From this the feature selector computes the length between the eyes, the vertical distance to the nose, the mouth and the horizontal distance to the ears.<br/><br/>\n", "\n", "<b>Step 3:</b>\n", "You enter the features extracted from images of guilty and innocent people into the perceptron algorithm and construct a classifier (represented by a vector/list of numbers).\n", "</div>\n", "\n", "The final system is then the 3-stage algorithm: Rotate, Feature Extract, Classify with weight from step 3.\n", "\n", "<b>Question 1: </b>Which of the steps is learning and which step is design?\n", "\n", "    Is step 1 learning or design?\n", "    Is step 2 learning or design?\n", "    Is step 3 learning or design?\n", "\n", "<b>Question 2:</b> Do you think our approach will work? Why/Why not?\n", "\n", "<b>Question 3: </b> Can you extend/alter the problem/algorithm to estimate the length of the prison time given?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Data that is not numbers\n", "### Question 1: Spam Filters\n", "You are given the task to design a spam filter and you will be using **Linear Classification** and the perceptron algorithm (since that is all we know yet).\n", "\n", "The input data consists of a list of (email, spam/not spam label), and each email is represented by a variable length text string. \n", "Can you train a spam filter using this data using the perceptron algorithm and if so how? What issues do you see and do you have any ideas how they could be adressed? (Remember the coding exercise)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Question 2: Categorical Features\n", "You are solving a problem with machine learning and have decided to use linear regression. \n", "One of the data features is categorical and has four unordered values: Apple, Banana, Grape, Mango. \n", "How could you use that feature in a linear regression model (or for linear classification)?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Problem 1.12 from 'Learning By Data'\n", "======================\n", "\n", "## By hand\n", "Given $y_1\\leq\\cdots\\leq y_n\\in \\mathbb{R}$ find $h\\in \\mathbb{R}$ that on average is closest to $y_1,\\ldots,y_n$ measured by squared distance (least squares). That is, \n", "$$\n", "h_\\textrm{mean} =\\textrm{arg}\\min_h \\sum_{i=1}^n (h-y_i)^2\n", "$$ \n", "<b>Question 1: </b>Show that\n", "$h_\\textrm{mean} = \\frac{1}{n} \\sum_{i=1}^n y_i$ is the minimizer. \n", "\n", "HINT: Computing the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">derivative</a> may be worth the time and strain on your brain.\n", "\n", "HINT: a local minimum is a global minimum!\n", "\n", "\n", "<b>Question 2: </b>Consider absolute deviation instead of squared distance, i.e.\n", "\n", "$$h_\\mathrm{med} =\\textrm{arg}\\min_h \\sum_{i=1}^n |h-y_i|$$ then the median $h_\\mathrm{med} = \\mathrm{median}(y_1,\\dots,y_n)$ is the minimizer. \n", "\n", "HINT: Computing derivative may be usefull but  $|a|$ is not differentiable at zero but you may set it to zero (ask google about subgradients if you are interested).  \n", "\n", "HINT: You can also argue purely algorithmically by thinking what happens with the cost as we sweep *h* from $-\\infty$ to $\\infty$).\n", "\n", "HINT: a local minimum is a global minimum!\n", "\n", "<b>Question 3: </b>What happens to the solutions $h_\\mathrm{mean}, h_\\mathrm{med}$ if we\n", "add noise the last element $y_n$, i.e. $y_n = y_n + \\varepsilon$ for\n", "$\\varepsilon \\rightarrow \\infty$.\n", "\n", "Which is more stable for outliers?\n", "\n", "\n", "See and run the code below for an experiment that tests the hypotheses"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# Enable plotting directly in the notebook\n", "%matplotlib inline\n", "# import NumPy and MatplotLib pyplot\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# sample 100 elements from normal distribution\n", "mean = 0\n", "std_dev = 1.5\n", "samples = 42\n", "y = np.random.lognormal(mean, std_dev, samples) # alex: changed from laplacian to normal. \n", "hs_opt = np.mean(y)\n", "ha_opt = np.median(y)\n", "\n", "# Plot the random points and the Mean/Median\n", "# Try run the code several times (CTRL + ENTER) and inspect the plot\n", "fig, ax = plt.subplots()\n", "ax.plot(y, np.zeros_like(y), 'x')\n", "mean, = plt.plot([hs_opt], [0], 'ro', ms=10, label='Mean')\n", "median, = plt.plot([ha_opt], [0], 'g^', ms=10, label='Median')\n", "ax.legend(handles=[mean, median])\n", "ax.axes.get_yaxis().set_visible(False)\n", "plt.show()\n", "\n", "# Compute and print the different costs. \n", "square_hs = np.sum((y-hs_opt)**2)\n", "square_ha = np.sum((y-ha_opt)**2)\n", "\n", "print('The found solutions')\n", "print('ha_opt:', ha_opt)\n", "print('hs_opt:', hs_opt)\n", "\n", "print('\\nSquared cost:')\n", "print('squared cost of ha_opt: \\t{0}'.format(round(square_ha,2)))\n", "print('squared cost of hs_opt: \\t{0}'.format(round(square_hs,2)))\n", "print('seems hs_opt is better for squared dist so far!')\n", "\n", "print('\\nAbsolute cost:')\n", "abs_hs =  np.sum(np.abs((y-hs_opt)))\n", "abs_ha =  np.sum(np.abs((y-ha_opt)))\n", "print('abs cost of ha_opt: \\t\\t{0}'.format(round(abs_ha, 2)))\n", "print('abs cost of hs_opt: \\t\\t{0}'.format(round(abs_hs, 2)))\n", "\n", "\n", "print('seems ha_opt is best for abs dist so far!')\n", "# Plot errors as function of h for both costs\n", "print('Lets see the cost functions evaluated at a grid and see if they also back up the claim')\n", "x_data = np.arange(-10, 10, 0.1)\n", "sq_costs = [np.sum((x - y)**2) for x in x_data]\n", "abs_costs = [np.sum(np.abs(x - y)) for x in x_data]\n", "fig, axes = plt.subplots(2, 1)\n", "axes[0].plot(x_data, sq_costs, 'b-', label='Squared cost of x')\n", "axes[0].set_title('Squared cost of x')\n", "axes[1].plot(x_data, abs_costs, 'b-', label='Abs. cost of x')\n", "axes[1].set_title('Absolute cost of x')\n", "plt.subplots_adjust(hspace=0.5)\n", "\n", "print('\\nYou can try to add noise to the largest element if you want - see perhaps np.argmax?')\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 6. Derivatives\n", "\n", "\n", "In Linear Regression we defined the in sample error as (ignoring the normalizing factor 1/n)\n", "\n", "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 $$ \n", "\n", "Let $X$ be the the data matrix of shape $n \\times d$ with data point $x_i$ as the $i$'th row. Let $y$ be the label vector of shape $n \\times 1$ with label $y_i$ as the $i$'th entry. Let $w$ be the weight vector of shape $d \\times 1$.  \n", "\n", "$$X=\\begin{pmatrix}\n", "- & x_1^T & - \\\\\n", "- & \\vdots & - \\\\\n", "- & x_n^T & - \\\\\n", "\\end{pmatrix}\\in \\mathbb{R}^{n \\times d}\\quad\\quad\\quad\n", "y=\\begin{pmatrix}\n", "y_1\\\\\n", "\\vdots\\\\\n", "y_n\n", "\\end{pmatrix}\\in \\mathbb{R}^{n \\times 1}$$\n", "\n", "The in-sample error rate $E_{in}$ is then equal to \n", "\n", "$$E_\\textrm{in}(w) = \\sum_{i=1}^{n} (w^\\intercal x_i - y_i)^2 =(Xw-y)^\\intercal (Xw-y)$$\n", "\n", "\n", "In the lecture we proved that for Linear Regression the optimal weight vector $w_\\textrm{lin}$ for minimizing $E_\\textrm{in}$ was $w_\\textrm{lin} = X^\\dagger y$ where $X^\\dagger=(X^\\intercal X)^{-1} X^\\intercal$ is the pseudoinverse. \n", "\n", "To do this we used facts about the Jacobian. \n", "* If we have a function $f(z): \\mathbb{R}^{a} \\rightarrow \\mathbb{R}^b$ such that $f(z) = [f_1(z),\\dots, f_b(z)]$ then the Jacobian is the matrix \n", "$$ \n", "J_{i,j} = \\frac{\\partial f_i}{\\partial z_j}\n", "$$ \n", "of size $b\\times a$. \n", "For example: Let $f(x): R^2 \\rightarrow R^3$ be the function $f([x_1,x_2]) = [x_1, x_2, x_1*x_2]$ then \n", "the Jacobian has shape $3 \\times 2$ and looks like\n", "\n", "$$\n", "J_f(x) =\n", " \\begin{bmatrix}\n", "  1 & 0 \\\\\n", "  0 & 1  \\\\\n", "  x_2  & x_1 \\\\\n", " \\end{bmatrix}\n", "$$\n", "\n", "In our proof we used the following identities about the Jacobian\n", "\n", "* $f: R^d \\rightarrow R^d, f(z) = Xz$, the Jacobian $J_f(z)$ is $X$\n", "* $g: R^d \\rightarrow R^d, g(z) = z-y$, the Jacobian $J_g(z)$ is $I$\n", "* $h: R^d \\rightarrow R, h(z) = z^\\intercal z$, the Jacobian $J_h(z)$ is $2z^\\intercal$\n", "\n", "Notice that $E_\\textrm{in} = h(g(f(w)))$. With these identities, we compute the gradient $\\nabla E_\\text{in}$ by multiplying the Jacobians of $h$, $g$, and $f$ evaluated at their inputs (abiding the mighty Chain Ruler), and take the transpose. https://en.wikipedia.org/wiki/Chain_rule\n", "\n", "Since $f(w) = Xw$, $g(f(w)) = g(Xw) = Xw-y $, and $h(g(f(w)) = (Xw-y)^\\intercal (Xw - y)$\n", "Then \n", "\n", "$$\n", "\\nabla E_\\textrm{in} = (2(Xw-y)^\\intercal I X)^\\intercal = 2X^\\intercal(Xw- y)\n", "$$\n", "\n", "Solving for the zero vector gives $w_\\textrm{lin}$ (and the 2 factor becomes irrelevant, like any initial 1/n scaling).\n", "\n", "**Your job is to prove the three identities. To simplify matters we have split the proof up in five parts:**\n", "\n", "In the following $x, w, z$ are columns vectors of shape $d \\times 1$, i.e. $x = [x_1,\\dots,x_d]^\\intercal$ and $X$ is a $d \\times d$ matrix\n", "* Let $f:\\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2}$, what is the shape of the Jacobian $J_f$\n", "* Let $f(x) = w^\\intercal x$, what is $\\frac{\\partial f}{\\partial x_1}$?, what is $\\frac{\\partial f}{\\partial x_j}$?. See a pattern. What is the Jacobian J_f?\n", "* Let $f(x) = x - z$, Show that the Jacobian of $f$ is the $d \\times d$ identity matrix.\n", "* Let $f(w) = Xw$  (function from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$). Show that the Jacobian $J_f = X$. Hint: You can think of $f = [f_1,\\dots,f_d]$ as d output functions where $f_i(w) = x_{i}^\\intercal w$ and $x_i$ is the i'th row of $X$. Start with $\\frac{\\partial f_1}{\\partial w_1}$ to see if a pattern emerges\n", "* Let $f(x) = x^\\intercal x$ (the squared norm of $x$). Show that the Jacobian $J_f = 2x^\\intercal$\n", "\n", "\n", "\n", "\n", "We have implemented that in python below to see if the shapes fit at least."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Python linear regression cost function and derivative - maybe just set random data points\n", "\n", "d = 2\n", "n = 3\n", "w = np.array([1, 2 ]).reshape(d, 1) # d x 1 vector\n", "X = np.array([[1,2], [1,3], [1,4]]) # n x d matrix\n", "y = np.array([2, 3, 5]).reshape(n, 1) # n x 1 vector\n", "print('Lets check the shapes: ', w.shape, X.shape, y.shape)\n", "print('Lets do the forward pass and compute Ein piece by piece')\n", "z1 = X@w  #Xw - outputs a vector ;; forward\n", "z2 = z1-y #outputs a vector\n", "z3 = z2.T @ z2 # outputs a scalar, Ein\n", "\n", "print('In sample error of w is z3 and is:', z3)\n", "print('The data X, y is fixed so z3 is a function of w only so to minimize we compute derivatives using the chain rule and return')\n", "print('The input is a function from R^d -> R - so the Jacobian should have shape 1 x d and the gradient d x 1')\n", "print('Lets use reverse the computation and compute derivatices by backpropagating and apply the chain rule')\n", "part_z3 = 2*z2.T # as defined the derivate of h(z) is 2z^t and we input z2, thus 2z2 ;; back\n", "part_z2 = np.eye(n) # The derivative of g was the the identity matrix of size n x n, the input is irrelevant\n", "part_z1 = X # the derivative of f was X regardless of input\n", "print('See if the shapes fit:', part_z1.shape, part_z2.shape, part_z3.shape)\n", "jacob_E = part_z3 @ part_z2 @ part_z1\n", "\n", "print('Shape of the Jacobiant:', jacob_E.shape)\n", "print('Shape of the Gradient:', jacob_E.T.shape)\n", "print('What is the Gradient: \\n', jacob_E.T)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 7. Cost Functions\n", "\n", "You work at a company, and you have just released a new system based on\n", "machine learning using linear regression with the least squares error\n", "measure. You did not have time to look much at the final product but in\n", "testing everything looked fine. One of your colleagues, Clumsy, was\n", "responsible for running the learning algorithm on the final data you\n", "collected.\n", "\n", "The following morning Clumsy comes to you and says: <br><br>\n", "\n", "<div style=\"margin-left: 16px; \">\n", "   *I made an error on the few datapoints we received yesterday. I forgot to scale them down as\n", "we did with all the other data. But it is less than 1 percent of the\n", "data, so it is probably not a problem \u2013 the other data will dominate,\n", "right, since the error measure is an average.* </div>\n", "\n", "<b>Is clumsy right?</b>\n", "\n", "If you need help or would like to see the experiment in action read and run the script in the cell below."]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# add python clumsy code\n", "from sklearn.linear_model import LinearRegression as lr \n", "\n", "\n", "w = np.array([3, 7])\n", "n = 300\n", "z = np.random.rand(n).reshape(-1, 1)\n", "z.sort()\n", "X = np.c_[np.ones(n), z]\n", "y = X @ w + np.random.rand(n)\n", "new_n = int(n/100)\n", "new_z = np.random.rand(new_n).reshape(-1, 1)\n", "new_z.sort()\n", "new_X = np.c_[np.ones(new_n), new_z]\n", "new_y = 42*(new_X @ w + np.random.rand(new_n)) # scaled by 42\n", "model1 = lr().fit(z, y)\n", "all_X = np.vstack((z, new_z))\n", "all_y = np.vstack((y.reshape(-1,1), new_y.reshape(-1, 1)))\n", "model2 = lr().fit(all_X, all_y.reshape(-1,))\n", "print('Model 1 Weights:', [model1.intercept_, model1.coef_[0]])\n", "print('Model 2 Weights:', [model2.intercept_, model2.coef_[0]])\n", "p1 = model1.predict(z)\n", "p2 = model2.predict(z)\n", "print('Model 1 Score (Error): ', np.mean((y-p1)**2))\n", "print('Model 2 Score (Error): ', np.mean((y-p2)**2))\n", "\n", "\n", "plt.figure(figsize=(10,8))\n", "plt.scatter(z, y, c='b', label='Good Data')\n", "# plt.scatter(new_z, new_y, c='g', label='New Unscaled Data')\n", "plt.plot(z, p1, 'r-', label='Model 1')\n", "plt.plot(z, p2, 'g-', label='Model 2')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["# 8. Linear Regression and Correlation Coefficient (Harder Exercise, for the ambitious)\n", "Consider data $D=\\{(x_1,y_1),\\dots, (x_n,y_n)\\}$ where $x_i\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}$. Remember that linear regression is given\n", "\n", "$$w_\\textrm{lin} = \\text{arg min}_w \\sum_{i=1}^n (w^\\intercal x_i + b - y_i)^2$$\n", "\n", "Linear Regression is described on page 84 in the book, it even has a nice image (Figure 3.3). You might wonder why the above formula has the bias $b$ term and the formula in the book doesn't. This is because of the notational trick described on page 5-7. In this exercise we consider the 1-dimensional case with $b=0$\n", "\n", "<b>Question 1: </b>Simplify $w_\\text{lin}=\\text{arg min}_w \\sum_{i=1}^n (w^\\intercal x_i + b - y_i)^2$ for the 1-dimensional with $b=0$.<br><br>\n", "HINT: The answer is given below, see $w_{1D-in}$. <br><br>\n", "\n", "<div style=\"border: 1px solid #333; padding:16px; margin: 16px;\"><b>Definition: </b>\n", "Given $x,y\\in \\mathbb{R}^n$ the (sample) Correlation Coefficient is defined as<br><br>\n", "\n", "$$\n", "CC(x,y)=\\frac{\\sum_{i=1}^n (x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n (x_i - \\mu_x)^2 \\sum_{i=1}^n (y_i - \\mu_y)^2}}\\quad\\quad \\text{where} \\quad \\quad \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x_i\\quad\\text{and}\\quad \\mu_y = \\frac{1}{n} \\sum_{i=1}^n y_i \n", "$$\n", "\n", "</div>\n", "\n", "Notice that $\\mu_x$ and $\\mu_y$ are the mean of $x$ and $y$ respectively. The (sample) Correlation Coefficient measures to what extent the two samples vary together (relative to the mean). If two vectors vary the same way it seems plausible that we can use one to estimate the other. \n", "\n", "But we already know how to predict one value from another using Linear Regression, so it seems that the (sample) Correlation Coeficient and Linear Regression somehow do something similar!\n", "\n", "<b>Question 2: </b> Prepare an explanation to your fellow students of why Linear Regression and the (sample) Correlation Coefficient seem to do something similar. \n", "\n", "\n", "The 1-dimensional Linear Regression (predict $y$ from number $x$) for bias $b=0$ considered in Question 1 boils down to \n", "$$\n", "w_\\textrm{1D-lin} = \\text{arg min}_w \\sum_{i=1}^n (w x_i - y_i)^2\n", "$$\n", "\n", "<b>Question 3: </b>\n", "Let $x, y\\in \\mathbb{R}^n$ and let $x', y'$ be the normalized versions (zero mean, std. deviation 1), that is \n", "\n", "$$x'_i = \\frac{(x_i - \\mu_x)}{\\sigma_x}\\quad\\quad \\text{where}\\quad\\quad  \\mu_x = \\frac{1}{n} \\sum_{i=1}^n x_i \\quad\\quad\\text{and}\\quad\\quad \\sigma_x = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu_x)^2}$$\n", "\n", "The same for $y'$. \n", "\n", "Show that the Linear Regression weight wector $w_\\textrm{1D-lin}$ computed on data x' and targets y' is the sample Correlation Coefficient of x and y\n", "\n", "$$ \\text{arg min}_w \\sum_{i=1}^n(w x_i' - y_i')^2 = CC(x', y')$$\n", "\n", "HINT: Show that given input vectors $x, y$ (use $x$ to predict $y$) then\n", "\n", "$$\n", "w_\\textrm{1D-lin} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = \\frac{\\langle x, y\\rangle}{\\langle x, x\\rangle} = \\frac{x^\\intercal y}{|x|^2}\n", "$$\n", "\n", "by taking the derivative, setting it equal to $0$ and solving for $w$ (as in the Lecture) - only that now it is a function of a single variable (vector of length one).\n", "Then plugin the definition for $x'$ and $y'$ and see if you can make it fit.\n", "\n", "see below for a code example"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["x = np.array([1,2,3,1,2,3,1,-1,-2,3,4,-3])\n", "y = 3*x -0.5\n", "plt.plot(x, y, 'r-x', linewidth=2)\n", "def compare(x, y):\n", "    w1dlin = np.dot(x,y)/np.dot(x, x)\n", "    mux = np.mean(x)\n", "    stdx = np.sqrt(np.mean((x-mux)**2))\n", "    muy = np.mean(y)\n", "    stdy = np.sqrt(np.mean((y-muy)**2))\n", "    xp = (x-mux)/stdx\n", "    yp = (y-muy)/stdy\n", "\n", "    correlation_coefficient = np.dot((x - mux),(y - muy)) / np.sqrt(np.dot(x - mux, x - mux)*np.dot(y - muy, y - muy))\n", "    print('Correlation coeffient of x and y:', correlation_coefficient)\n", "    w1d_lin = np.dot(xp, yp)/np.dot(xp, xp)\n", "    print(\"w1d_lin of x', y': \", w1d_lin)\n", "\n", "print('compare ', x, y)\n", "compare(x, y)\n", "y = np.array([-1, -2, -3, -1, -2, -3, -1, -1, -2, -3, -4, -3])\n", "print('compare', x, y)\n", "compare(x, y) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}